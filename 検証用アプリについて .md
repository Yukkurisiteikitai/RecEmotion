# 検証用アプリについて

開発ロードマップとして、フェーズごとにタスクを細分化しました。
これをコピーして、NotionやGitHub Issuesなどのタスク管理ツールに貼り付けて使ってください。

特に**Phase 1**が、現在の「API Level 15問題」を解決し、MediaPipeを動かすための最優先事項です。

---

### Phase 1: 環境構築と基盤整備 (Environment Setup)

まず、MediaPipeが動く土台を作ります。

- [ ]  **Androidプロジェクトの設定変更**
    - `build.gradle (Module: app)` の `minSdk` を `15` から **`24` (推奨: 26)** に変更する。
    - MediaPipe Face Landmarker の依存関係 (`implementation`) を追加/確認する。
- [ ]  **Rustプロジェクトの依存関係追加**
    - `Cargo.toml` に以下のクレートを追加する。
        - `jni` (JNI連携用)
        - `serde`, `serde_json` (JSONシリアライズ用)
        - `lazy_static` または `once_cell` (グローバル状態管理用)
        - `chrono` (時間計算用)
- [ ]  **権限周りの設定**
    - `AndroidManifest.xml` にカメラ権限 (`CAMERA`) を追加する。

### Phase 2: カメラと表情認識の実装 (Kotlin/Android)

まだRustには繋がず、Android側だけで「数値」が取れるか確認します。

- [ ]  **CameraXの導入**
    - プレビュー画面 (`PreviewView`) をレイアウトに配置する。
    - カメラのライフサイクル管理とプレビュー表示を実装する。
- [ ]  **MediaPipe Face Landmarkerの実装**
    - `FaceLandmarker` インスタンスを生成する。
    - カメラフレームをMediaPipeに流し込む処理を実装する。
    - コールバックで `FaceLandmarkerResult` を受け取る。
- [ ]  **Blendshapesデータの抽出**
    - 結果から `faceBlendshapes` (52個のFloat値) を取り出す。
    - Logcatに「Happy: 0.8」などを出力して動作確認する。

### Phase 3: JNIブリッジの構築 (Kotlin <-> Rust)

KotlinからRustへデータを渡すパイプラインを通します。

- [ ]  **JNIインターフェース定義 (Kotlin)**
    - `external fun initSession(wakeTime: Long)`
    - `external fun pushEmotionFrame(scores: FloatArray)`
    - `external fun getAnalysisJson(text: String): String`
    - `external fun updateStressLevel(level: Int)`
- [ ]  **JNI関数実装 (Rust)**
    - Kotlinの定義に対応する `extern "C"` 関数を `lib.rs` に書く。
    - 受け取ったデータを単に `println!` (Android Log) するだけの仮実装で、データ到達を確認する。

### Phase 4: 分析ロジックの実装 (Rust Core)

「脳みそ」部分を作ります。

- [ ]  **状態管理 (State Management)**
    - `SessionState` 構造体を定義する（起床時間、ストレス値、感情データの配列などを保持）。
    - `Mutex` と `lazy_static` を使い、この構造体をグローバルに保持する仕組みを作る。
- [ ]  **体力計算ロジックの実装**
    - 起床時間 (`wakeTime`) と現在時刻から、経過時間を計算して `Energy Level (1-5)` を返す関数を書く。
- [ ]  **表情データの集計ロジック**
    - 毎フレーム送られてくる `Vec<f32>` を蓄積する。
    - リクエスト時に、それらの「平均値」または「最大頻度」を計算する処理を書く。
- [ ]  **JSON生成 (Prompt Builder)**
    - `serde_json` を使い、以前設計したフォーマット（Status, Context, EmotionData）のJSON文字列を生成して返す関数を実装する。

### Phase 5: UI実装と統合 (Integration)

アプリとして使える形にします。

- [ ]  **入力UIの作成**
    - **設定画面:** 「起床時間（TimePicker / 数値入力）」と「ストレス値（Slider 1-5）」の入力フォーム。
    - **メイン画面:** テキスト入力欄（今日の振り返り用）と「分析開始」ボタン。
- [ ]  **通知連携 (Optional/Android)**
    - 他のアプリ（MacroDroid等）の通知から起床時間を取得する場合、その値を入力欄に反映するロジック（または手動入力）。
- [ ]  **LLM API連携 (Kotlin)**
    - Rustから受け取ったJSON文字列を含めて、OpenAI API (またはClaude API) にHTTPリクエストを送る処理を実装する（OkHttp / Retrofit）。
    - 返ってきた分析結果を画面に表示する。

### Phase 6: 実験と検証 (Validation)

実際に使ってデータを取ります。

- [ ]  **動作テスト (Dry Run)**
    - 意図的に変な顔をして、JSONの数値が変わるか確認する。
    - ストレス値をMAXにして、LLMの出力が変わるか確認する。
- [ ]  **本番運用 (3日間〜)**
    - 毎晩アプリを使い、ログ（JSONとLLMの回答）を保存する。
    - スプレッドシートの評価シートを記入する。

---

まずは **Phase 1 (minSdkの変更)** と **Phase 2 (Kotlinで顔認識)** をクリアすることを目指しましょう！そこさえ超えれば、後はRustでロジックを書くだけ（Nikeriさんの得意領域）になります。

```
rustで描き切る必要性はあまり感じないなこ正直、ステート管理に関してはKotlinが対応するし
rustは.soのただ実行することだけする関数的な利用を想定している仕様だから
あんまり本格的にそのデータを打ち込み続けるわけにも行かない。
だからrustは表情分析の計算に限定した利用になっているから、
それ以外のやつはそこまで感がなくてもいいかもしれない。
でそれを持ってPhase 4の分析ロジックの実装についてだけど
まうzState managementをする必要性がある

それで持つべきデータは
 - 起床時間
 - ストレス値
 - 感情データ
 を保持することが可能である。
これをどうやって保管するのかっていうことで、matexとlazy_static を使ってandroidのアプリ内の共通変数として扱えるようにするっていうことかな？
できるんかわからんなこれ

```