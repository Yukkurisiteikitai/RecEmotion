# Androidの実行順序について

そもそも私はAndroidの実行順序を正しく理解していない
まずどうやって実行されるの？

1. ホームでアイコンをタップ
2. Linuxプロセス作成
3. ART（Android Runtime）起動
4. アプリのVM生成
5. 権限要求の処理が読み込まれる（AndroidManifest.xml）
6. Application.onCreate()の実行
7. buttonの作成をしていく?
8. Eventの購買
みたいな感じだったかな

大体そんな感じでセットアップされるはず


---

# RecEmotionプロジェクトの実行順序

## エントリーポイント

`AndroidManifest.xml` にて `MainActivity` がLAUNCHERとして指定されている。
カスタムApplicationクラスは存在しないため、デフォルトのApplicationが使われる。

---

## 時系列で見る初期化フロー

### T+0: Androidがアプリを起動

OS が `MainActivity` を起動する。

---

### T+1: `MainActivity` のクラスロード → `librecemotion.so` のロード

`MainActivity.kt` の `companion object { init { ... } }` ブロックが、
クラスがはじめてロードされた瞬間に実行される。

```kotlin
// MainActivity.kt
companion object {
    init {
        System.loadLibrary("recemotion")  // Rustライブラリをロード
    }

    @JvmStatic external fun initSession(wakeTime: Long)
    @JvmStatic external fun pushFaceLandmarks(landmarks: FloatArray)
    @JvmStatic external fun getAnalysisJson(text: String): String
    @JvmStatic external fun updateStressLevel(level: Int)
}
```

**ポイント:** `System.loadLibrary()` はJNIの仕組みで呼ばれ、
Rustライブラリ内の `JNI_OnLoad` が自動的に呼ばれる。

```rust
// lib.rs
pub extern "system" fn JNI_OnLoad(_vm: *mut JavaVM, ...) -> jint {
    // Androidロガーを初期化
    android_logger::init_once(...);
    jni::sys::JNI_VERSION_1_6  // JNIバージョンを返す
}
```

---

### T+2: `MainActivity.onCreate()` が実行される

```kotlin
// MainActivity.kt
override fun onCreate(savedInstanceState: Bundle?) {
    super.onCreate(savedInstanceState)
    binding = ActivityMainBinding.inflate(layoutInflater)  // レイアウトをインフレート
    setContentView(binding.root)

    supportActionBar?.hide()

    setupNavigation()    // ナビゲーションドロワーのセットアップ
    setupSwipeGesture()  // スワイプジェスチャーのセットアップ

    if (savedInstanceState == null) {
        // MainScreenFragment と CalendarFragment を追加
        // CalendarFragment は非表示（hide）
        supportFragmentManager.beginTransaction()
            .add(R.id.fragmentContainer, MainScreenFragment(), TAG_MAIN)
            .add(R.id.fragmentContainer, CalendarFragment(), TAG_CALENDAR)
            .hide(calFrag)
            .commit()
    }
}
```

最初に表示されるのは `MainScreenFragment`。

---

### T+3: `MainScreenFragment.onViewCreated()` が実行される

ここが一番重要な初期化の場所。

```kotlin
// MainScreenFragment.kt
override fun onViewCreated(view: View, savedInstanceState: Bundle?) {
    super.onViewCreated(view, savedInstanceState)

    // 1. 各種ヘルパーを初期化
    cameraExecutor = Executors.newSingleThreadExecutor()
    faceLandmarkerHelper = FaceLandmarkerHelper(...)
    llmInferenceHelper = LLMInferenceHelper(requireContext())
    modelDownloadHelper = ModelDownloadHelper(requireContext())
    thoughtAnalysisViewModel = createThoughtAnalysisViewModel()

    setupUI()  // 2. UIリスナーのセットアップ

    // 3. Rustのセッションを初期化（起床時刻を渡す）
    val wakeTimeUnix = ...  // 今日の07:00をUnix時間に変換
    MainActivity.initSession(wakeTimeUnix)  // → Rustの init_session() が呼ばれる

    checkAndDownloadModel()  // 4. LLMモデルの確認・ダウンロード

    // 5. カメラ権限を確認してカメラ起動
    if (カメラ権限あり) startCamera() else requestPermissionLauncher.launch(...)

    // 6. コルーチンでLLM・感情分析の結果を監視開始
    collectLlmResults()
    collectLlmProgress()
    collectThoughtAnalysisState()

    // 7. CaboChaのネイティブパーサーを非同期で初期化
    viewLifecycleOwner.lifecycleScope.launch {
        initNativeParser()  // libcabocha_jni.so のロードはここで起きる
    }
}
```

---

### T+4: Rustで `init_session()` が実行される

```rust
// core.rs
pub fn init_session(wake_time: i64) {
    let mut state = GLOBAL_STATE.lock().unwrap();
    state.wake_time = Some(wake_time);
    state.calibrator = StatisticalCalibrator::new();  // キャリブレーターをリセット
    state.emotion_history.clear();
    state.current_emotion = "Neutral".to_string();
}
```

グローバルな `Mutex<SessionState>` を初期化する。

---

### T+5: カメラ起動 → フレーム処理ループ開始

```
startCamera()
  → CameraX ImageAnalysis を設定
  → FaceLandmarkerHelper.detectLiveStream() でフレームを処理
  → onResults() コールバックが毎フレーム（約30回/秒）呼ばれる
```

`onResults()` の中でJNIが呼ばれる:

```kotlin
// MainScreenFragment.kt
override fun onResults(result: FaceLandmarkerResult, inferenceTime: Long) {
    val flattened = FloatArray(...)  // 468点 × 3軸 の顔ランドマーク

    MainActivity.pushFaceLandmarks(flattened)       // → Rust: process_face_landmarks()
    val jsonStr = MainActivity.getAnalysisJson("")  // → Rust: generate_analysis_json()
    requireActivity().runOnUiThread { updateUI(jsonStr) }
}
```

---

### T+6（並列・バックグラウンド）: `libcabocha_jni.so` のロードと辞書インストール

`NativeCabochaParser` クラスが初めてアクセスされたとき、
`companion object { init { System.loadLibrary("cabocha_jni") } }` が実行される。

```kotlin
// NativeCabochaParser.kt
private suspend fun initNativeParser() {
    if (!dictionaryManager.isInstalled()) {
        dictionaryManager.install()  // MeCab辞書（約51MB）をインストール
    }
    nativeParser = NativeCabochaParser(mecabDicDir = dictionaryManager.dictPath)
    nativeParser!!.nativeVerify(...)  // 辞書が正常に読めるか確認（0=OK）
}
```

---

## Rustの状態機械（感情分析のコア）

フレームごとに呼ばれる `process_face_landmarks()` の内部処理:

```
1. 頭部姿勢のバリデーション（正面を向いているか）
2. 468点の顔ランドマークから幾何学的特徴量を抽出
3. キャリブレーション済みでなければ → サンプルを蓄積
4. キャリブレーション済みなら → calculate_emotion() で感情を計算
```

`getAnalysisJson()` が返すJSON:
- `current_emotion` : 現在の感情
- `energy_level` : 起床時刻からの経過時間をもとにしたエネルギーレベル
- `stress_level` : ストレスレベル（UIのスライダーで手動更新可）
- `is_calibrated` : キャリブレーション完了しているか
- `sample_count` : 蓄積済みサンプル数

---

## JNIの呼び出しタイミングまとめ

| ライブラリ | ロードタイミング | 場所 |
|---|---|---|
| `librecemotion.so` (Rust) | `MainActivity` クラスロード時（アプリ起動直後） | `MainActivity.kt` companion object init |
| `libcabocha_jni.so` (C++) | `NativeCabochaParser` 初回アクセス時（遅延ロード） | `NativeCabochaParser.kt` companion object init |

| JNI関数 | 呼ばれるタイミング | スレッド |
|---|---|---|
| `initSession()` | `onViewCreated()` 内 | メインスレッド |
| `pushFaceLandmarks()` | カメラフレームごと（30回/秒） | メインスレッド（runOnUiThread経由） |
| `getAnalysisJson()` | `pushFaceLandmarks()` の直後 | メインスレッド（runOnUiThread経由） |
| `updateStressLevel()` | UIのスライダー操作時 | メインスレッド |
| `nativeVerify()` | 起動時の辞書確認 | コルーチン（バックグラウンド） |
| `nativeParse()` | テキスト解析実行時 | コルーチン（バックグラウンド） |
